import torch
import torch.nn as nn
from torch.nn.functional import relu6


# è®ºæ–‡åœ°å€ï¼šhttps://ieeexplore.ieee.org/abstract/document/10504297
# DGMA2 - Net: A Difference - Guided Multiscale Aggregation Attention Network for Remote Sensing Change Detectio

# Multi-Scale Feature Fusion (MSFF) æ¨¡å—ï¼šç”¨äºå¤šå°ºåº¦ç‰¹å¾èåˆçš„æ¨¡å—
class MSFF(nn.Module):
    def __init__(self, inchannel, mid_channel):
        super(MSFF, self).__init__()

        # å·ç§¯1x1ï¼Œä¿æŒè¾“å…¥é€šé“æ•°ä¸å˜ï¼Œæ¿€æ´»å‡½æ•°ä½¿ç”¨ReLU
        self.conv1 = nn.Sequential(
            nn.Conv2d(inchannel, inchannel, 1, stride=1, bias=False),
            nn.BatchNorm2d(inchannel),
            nn.ReLU(inplace=True)
        )

        # å·ç§¯1x1 + 3x3å·ç§¯ï¼Œæ‰©å¤§ç‰¹å¾å›¾çš„é€šé“æ•°ï¼Œç»è¿‡ä¸­é—´å±‚åå†é™å›åŸé€šé“æ•°
        self.conv2 = nn.Sequential(
            nn.Conv2d(inchannel, mid_channel, 1, stride=1, bias=False),
            nn.BatchNorm2d(mid_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channel, mid_channel, 3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(mid_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channel, inchannel, 1, stride=1, bias=False),
            nn.BatchNorm2d(inchannel),
            nn.ReLU(inplace=True)
        )

        # å·ç§¯1x1 + 5x5å·ç§¯ï¼Œå†ç»è¿‡1x1å·ç§¯
        self.conv3 = nn.Sequential(
            nn.Conv2d(inchannel, mid_channel, 1, stride=1, bias=False),
            nn.BatchNorm2d(mid_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channel, mid_channel, 5, stride=1, padding=2, bias=False),
            nn.BatchNorm2d(mid_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channel, inchannel, 1, stride=1, bias=False),
            nn.BatchNorm2d(inchannel),
            nn.ReLU(inplace=True)
        )

        # å·ç§¯1x1 + 7x7å·ç§¯ï¼Œå†ç»è¿‡1x1å·ç§¯
        self.conv4 = nn.Sequential(
            nn.Conv2d(inchannel, mid_channel, 1, stride=1, bias=False),
            nn.BatchNorm2d(mid_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channel, mid_channel, 7, stride=1, padding=3, bias=False),
            nn.BatchNorm2d(mid_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channel, inchannel, 1, stride=1, bias=False),
            nn.BatchNorm2d(inchannel),
            nn.ReLU(inplace=True)
        )

        # å°†å¤šå°ºåº¦çš„ç‰¹å¾å›¾ï¼ˆå››ä¸ªå·ç§¯çš„è¾“å‡ºï¼‰æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå¹¶è¿›è¡Œèåˆå·ç§¯
        self.convmix = nn.Sequential(
            nn.Conv2d(4 * inchannel, inchannel, 1, stride=1, bias=False),
            nn.BatchNorm2d(inchannel),
            nn.ReLU(inplace=True),
            nn.Conv2d(inchannel, inchannel, 3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(inchannel),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        # å››ä¸ªä¸åŒå°ºåº¦çš„å·ç§¯æ“ä½œ
        x1 = self.conv1(x)
        x2 = self.conv2(x)
        x3 = self.conv3(x)
        x4 = self.conv4(x)

        # å°†ä¸åŒå°ºåº¦çš„ç‰¹å¾å›¾æ‹¼æ¥
        x_f = torch.cat([x1, x2, x3, x4], dim=1)

        # é€šè¿‡èåˆå·ç§¯å¾—åˆ°è¾“å‡ºç‰¹å¾å›¾
        out = self.convmix(x_f)

        return out


# è‡ªåŠ¨å¡«å……å‡½æ•°ï¼šç¡®ä¿å·ç§¯æ“ä½œåè¾“å‡ºçš„ç‰¹å¾å›¾å°ºå¯¸ä¸è¾“å…¥ä¸€è‡´
def autopad(k, p=None, d=1):  # kernel, padding, dilation
    """Pad to 'same' shape outputs."""
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p


# æ ‡å‡†å·ç§¯æ¨¡å—ï¼šåŒ…æ‹¬å·ç§¯ã€æ‰¹é‡å½’ä¸€åŒ–ã€æ¿€æ´»å‡½æ•°ç­‰
class Conv(nn.Module):
    """Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)."""

    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        """Initialize Conv layer with given arguments including activation."""
        super().__init__()
        # å®šä¹‰å·ç§¯å±‚
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        # å®šä¹‰æ‰¹é‡å½’ä¸€åŒ–
        self.bn = nn.BatchNorm2d(c2)
        # é»˜è®¤æ¿€æ´»å‡½æ•°ï¼ˆSiLUï¼‰æˆ–ç”¨æˆ·è‡ªå®šä¹‰çš„æ¿€æ´»å‡½æ•°
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

    def forward(self, x):
        """Apply convolution, batch normalization and activation to input tensor."""
        # å·ç§¯ã€æ‰¹é‡å½’ä¸€åŒ–å’Œæ¿€æ´»å‡½æ•°çš„é¡ºåºæ“ä½œ
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        """Perform transposed convolution of 2D data."""
        # åªåšå·ç§¯æ“ä½œï¼ˆæ— æ‰¹é‡å½’ä¸€åŒ–ã€æ¿€æ´»å‡½æ•°ï¼‰
        return self.act(self.conv(x))


# å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—ï¼ˆMDFMï¼‰
class MDFM(nn.Module):
    def __init__(self, in_d, out_d):
        super(MDFM, self).__init__()
        self.in_d = in_d
        self.out_d = out_d
        # å®šä¹‰å¤šå°ºåº¦ç‰¹å¾èåˆï¼ˆMSFFï¼‰æ¨¡å—
        self.MPFL = MSFF(inchannel=in_d, mid_channel=64)  ##64

        # å·®å¼‚å¢å¼ºå·ç§¯ï¼šå¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œå¢å¼º
        self.conv_diff_enh = nn.Sequential(
            nn.Conv2d(self.in_d, self.in_d, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(self.in_d),
            nn.ReLU(inplace=True)
        )

        # å·®å¼‚å¢å¼ºåçš„å·ç§¯æ“ä½œï¼Œå¾—åˆ°è¾“å‡ºç‰¹å¾å›¾
        self.conv_dr = nn.Sequential(
            nn.Conv2d(self.in_d, self.out_d, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(self.out_d),
            nn.ReLU(inplace=True)
        )

        # å·®å¼‚è®¡ç®—å·ç§¯ï¼šè®¡ç®—è¾“å…¥ç‰¹å¾å›¾ä¹‹é—´çš„å·®å¼‚
        self.conv_sub = nn.Sequential(
            nn.Conv2d(self.in_d, self.in_d, 3, padding=1, bias=False),
            nn.BatchNorm2d(self.in_d),
            nn.ReLU(inplace=True),
        )

        # èåˆå·ç§¯ï¼šå°†å·®å¼‚ç‰¹å¾å›¾è¿›è¡Œèåˆ
        self.convmix = nn.Sequential(
            nn.Conv2d(2 * self.in_d, self.in_d, 3, groups=self.in_d, padding=1, bias=False),
            nn.BatchNorm2d(self.in_d),
            nn.ReLU(inplace=True),
        )

        # å·ç§¯ä¸Šé‡‡æ ·ï¼šå¯¹è¾“å…¥ç‰¹å¾å›¾è¿›è¡Œä¸Šé‡‡æ ·
        self.conv_up = Conv(int(in_d * 0.5), in_d, 1, act=nn.ReLU())

    def forward(self, x):
        # è¾“å…¥xæ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªç‰¹å¾å›¾çš„å…ƒç»„ï¼Œx[0]å’Œx[1]
        x1, x2 = x[0], x[1]
        b, c, h, w = x1.shape[0], x1.shape[1], x1.shape[2], x1.shape[3]

        # å¯¹x2è¿›è¡Œä¸Šé‡‡æ ·
        x2 = self.conv_up(x2)

        # è®¡ç®—ç‰¹å¾å›¾ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶ä½¿ç”¨å·ç§¯æ¨¡å—è¿›è¡Œå¤„ç†
        x_sub = torch.abs(x1 - x2)
        x_att = torch.sigmoid(self.conv_sub(x_sub))

        # å¯¹x1å’Œx2è¿›è¡Œå·®å¼‚å¢å¼º
        x1 = (x1 * x_att) + self.MPFL(self.conv_diff_enh(x1))
        x2 = (x2 * x_att) + self.MPFL(self.conv_diff_enh(x2))

        # ç‰¹å¾å›¾èåˆï¼šå°†x1å’Œx2å †å åˆ°ä¸€èµ·
        x_f = torch.stack((x1, x2), dim=2)
        x_f = torch.reshape(x_f, (b, -1, h, w))
        x_f = self.convmix(x_f)

        # æ ¹æ®æ³¨æ„åŠ›æœºåˆ¶ï¼ˆx_attï¼‰å¯¹èåˆåçš„ç‰¹å¾å›¾è¿›è¡ŒåŠ æƒ
        x_f = x_f * x_att

        # æœ€ç»ˆå·ç§¯æ“ä½œï¼Œå¾—åˆ°è¾“å‡ºç‰¹å¾å›¾
        out = self.conv_dr(x_f)

        return out


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    # åˆ›å»ºä¸¤ä¸ªè¾“å…¥ç‰¹å¾å›¾
    x1 = torch.randn((32, 512, 8, 8))  # æ‰¹æ¬¡å¤§å°32ï¼Œé€šé“æ•°512ï¼Œç‰¹å¾å›¾å¤§å°8x8
    x2 = torch.randn((32, 256, 8, 8))  # æ‰¹æ¬¡å¤§å°32ï¼Œé€šé“æ•°256ï¼Œç‰¹å¾å›¾å¤§å°8x8

    # åˆ›å»ºMDFMæ¨¡å‹
    model = MDFM(512, 64)  # è¾“å…¥é€šé“512ï¼Œè¾“å‡ºé€šé“64

    # å‰å‘ä¼ æ’­ï¼Œå¾—åˆ°è¾“å‡º
    out = model((x1, x2))

    # æ‰“å°è¾“å‡ºçš„å½¢çŠ¶
    print(out.shape)



# # Ultralytics YOLO ğŸš€, AGPL-3.0 license
# # YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
#
# # Parameters
# nc: 80 # number of classes
# scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
#   # [depth, width, max_channels]
#   n: [0.33, 0.25, 1024] # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
#   s: [0.33, 0.50, 1024] # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
#   m: [0.67, 0.75, 768] # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
#   l: [1.00, 1.00, 512] # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
#   x: [1.00, 1.25, 512] # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
#
# # YOLOv8.0n backbone
# backbone:
#   # [from, repeats, module, args]
#   - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2
#   - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4
#   - [-1, 3, C2f, [128, True]]
#   - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8
#   - [-1, 6, C2f, [256, True]]
#   - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16
#   - [-1, 6, C2f, [512, True]]
#   - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32
#   - [-1, 3, C2f, [1024, True]]
#   - [-1, 1, SPPF, [1024, 5]] # 9
#
# # YOLOv8.0n head
# head:
#   - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
#   - [[-1, 6], 1,MDFM, [256,384]]
#   - [-1, 3, C2f, [512]] # 12
#
#   - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
#   - [[-1, 4], 1, Concat, [1]] # cat backbone P3
#   - [-1, 3, C2f, [256]] # 15 (P3/8-small)
#
#   - [-1, 1, Conv, [256, 3, 2]]
#   - [[-1, 12], 1, Concat, [1]] # cat head P4
#   - [-1, 3, C2f, [512]] # 18 (P4/16-medium)
#
#   - [-1, 1, Conv, [512, 3, 2]]
#   - [[-1, 9], 1, Concat, [1]] # cat head P5
#   - [-1, 3, C2f, [1024]] # 21 (P5/32-large)
#
#   - [[15, 18, 21], 1, Detect, [nc]] # Detect(P3, P4, P5)

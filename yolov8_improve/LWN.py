import torch
from torch import nn
import pywt
from typing import Sequence, Tuple, Union, List
from einops import rearrange, repeat
import torch.nn.functional as F


# ç”¨äºè·å–æ»¤æ³¢å™¨å¼ é‡çš„å‡½æ•°
def get_filter_tensors(
        wavelet,
        flip: bool,
        device: Union[torch.device, str] = 'cpu',
        dtype: torch.dtype = torch.float32,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """å°†è¾“å…¥çš„waveletè½¬æ¢ä¸ºæ»¤æ³¢å™¨å¼ é‡ã€‚

    å‚æ•°ï¼š
        wavelet (Wavelet æˆ– str): ä¸€ä¸ªpywt waveletå…¼å®¹çš„å¯¹è±¡ï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªpywt waveletçš„åç§°ã€‚
        flip (bool): å¦‚æœä¸ºTrueï¼Œæ»¤æ³¢å™¨å°†è¢«ç¿»è½¬ã€‚
        device (torch.device): PyTorchç›®æ ‡è®¾å¤‡ã€‚é»˜è®¤å€¼ä¸º'cpu'ã€‚
        dtype (torch.dtype): æ•°æ®ç±»å‹ï¼Œè®¾ç½®è®¡ç®—çš„ç²¾åº¦ã€‚é»˜è®¤æ˜¯torch.float32ã€‚

    è¿”å›ï¼š
        tuple: è¿”å›åŒ…å«å››ä¸ªæ»¤æ³¢å™¨å¼ é‡çš„å…ƒç»„ï¼ˆdec_lo, dec_hi, rec_lo, rec_hiï¼‰ã€‚
    """
    wavelet = _as_wavelet(wavelet)

    # ç”¨äºåˆ›å»ºæ»¤æ³¢å™¨å¼ é‡çš„è¾…åŠ©å‡½æ•°
    def _create_tensor(filter: Sequence[float]) -> torch.Tensor:
        if flip:
            # å¦‚æœéœ€è¦ç¿»è½¬æ»¤æ³¢å™¨
            if isinstance(filter, torch.Tensor):
                return filter.flip(-1).unsqueeze(0).to(device)
            else:
                return torch.tensor(filter[::-1], device=device, dtype=dtype).unsqueeze(0)
        else:
            # ä¸ç¿»è½¬æ»¤æ³¢å™¨
            if isinstance(filter, torch.Tensor):
                return filter.unsqueeze(0).to(device)
            else:
                return torch.tensor(filter, device=device, dtype=dtype).unsqueeze(0)

    # è·å–å°æ³¢æ»¤æ³¢å™¨
    dec_lo, dec_hi, rec_lo, rec_hi = wavelet.filter_bank
    dec_lo_tensor = _create_tensor(dec_lo)
    dec_hi_tensor = _create_tensor(dec_hi)
    rec_lo_tensor = _create_tensor(rec_lo)
    rec_hi_tensor = _create_tensor(rec_hi)
    return dec_lo_tensor, dec_hi_tensor, rec_lo_tensor, rec_hi_tensor


# å°†æ³¢å½¢è½¬æ¢ä¸ºpywt waveletå¯¹è±¡çš„è¾…åŠ©å‡½æ•°
def _as_wavelet(wavelet):
    if isinstance(wavelet, str):
        return pywt.Wavelet(wavelet)
    else:
        return wavelet


# ShuffleBlockæ¨¡å—ï¼Œç”¨äºé€šé“é‡æ’
class ShuffleBlock(nn.Module):
    def __init__(self, groups=2):
        super(ShuffleBlock, self).__init__()
        self.groups = groups

    def forward(self, x):
        # é‡æ’å¼ é‡çš„ç»´åº¦
        x = rearrange(x, 'b (g f) h w -> b g f h w', g=self.groups)
        x = rearrange(x, 'b g f h w -> b f g h w')
        x = rearrange(x, 'b f g h w -> b (f g) h w')
        return x


# è®¡ç®—å¤–ç§¯çš„è¾…åŠ©å‡½æ•°
def _outer(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """torchå®ç°çš„numpyçš„outerå‡½æ•°ï¼Œç”¨äºè®¡ç®—1Då‘é‡çš„å¤–ç§¯ã€‚"""
    a_flat = torch.reshape(a, [-1])
    b_flat = torch.reshape(b, [-1])
    a_mul = torch.unsqueeze(a_flat, dim=-1)
    b_mul = torch.unsqueeze(b_flat, dim=0)
    return a_mul * b_mul


# æ„é€ äºŒç»´æ»¤æ³¢å™¨çš„å‡½æ•°
def construct_2d_filt(lo, hi) -> torch.Tensor:
    """é€šè¿‡å¤–ç§¯æ„é€ äºŒç»´æ»¤æ³¢å™¨ã€‚

    å‚æ•°ï¼š
        lo (torch.Tensor): ä½é€šæ»¤æ³¢å™¨ã€‚
        hi (torch.Tensor): é«˜é€šæ»¤æ³¢å™¨ã€‚

    è¿”å›ï¼š
        torch.Tensor: å››ä¸ªäºŒç»´æ»¤æ³¢å™¨çš„å †å ï¼ˆll, lh, hl, hhï¼‰ã€‚
    """
    ll = _outer(lo, lo)
    lh = _outer(hi, lo)
    hl = _outer(lo, hi)
    hh = _outer(hi, hi)
    filt = torch.stack([ll, lh, hl, hh], 0)
    return filt


# è®¡ç®—å¡«å……å¤§å°çš„å‡½æ•°
def _get_pad(data_len: int, filt_len: int) -> Tuple[int, int]:
    """è®¡ç®—æ‰€éœ€çš„å¡«å……é‡ã€‚

    å‚æ•°ï¼š
        data_len (int): è¾“å…¥æ•°æ®çš„é•¿åº¦ã€‚
        filt_len (int): æ»¤æ³¢å™¨çš„é•¿åº¦ã€‚

    è¿”å›ï¼š
        tuple: è¦åŠ åœ¨è¾“å…¥æ•°æ®ä¸¤è¾¹çš„å¡«å……é‡ã€‚
    """
    padr = (2 * filt_len - 3) // 2
    padl = (2 * filt_len - 3) // 2

    # å¯¹äºå¥‡æ•°é•¿åº¦çš„æ•°æ®ï¼Œå³è¾¹å¡«å……1
    if data_len % 2 != 0:
        padr += 1

    return padr, padl


# å¯¹æ•°æ®è¿›è¡Œ2D FWTå¡«å……çš„å‡½æ•°
def fwt_pad2(
        data: torch.Tensor, wavelet, mode: str = "replicate"
) -> torch.Tensor:
    """å¯¹æ•°æ®è¿›è¡Œ2D FWTå¡«å……ã€‚

    å‚æ•°ï¼š
        data (torch.Tensor): è¾“å…¥æ•°æ®ï¼ˆ4ç»´ï¼‰ã€‚
        wavelet (Wavelet æˆ– str): pywt waveletå¯¹è±¡æˆ–waveletåç§°ã€‚
        mode (str): å¡«å……æ¨¡å¼ï¼ˆæ”¯æŒ'reflect', 'zero', 'constant', 'periodic'ç­‰æ¨¡å¼ï¼Œé»˜è®¤ä¸º'replicate'ï¼‰ã€‚

    è¿”å›ï¼š
        torch.Tensor: å¡«å……åçš„æ•°æ®ã€‚
    """
    wavelet = _as_wavelet(wavelet)
    padb, padt = _get_pad(data.shape[-2], len(wavelet.dec_lo))
    padr, padl = _get_pad(data.shape[-1], len(wavelet.dec_lo))

    # ä½¿ç”¨PyTorchçš„padå‡½æ•°è¿›è¡Œå¡«å……
    data_pad = F.pad(data, [padl, padr, padt, padb], mode=mode)
    return data_pad


# DWTï¼ˆç¦»æ•£å°æ³¢å˜æ¢ï¼‰æ¨¡å—
class DWT(nn.Module):
    def __init__(self, dec_lo, dec_hi, wavelet='haar', level=1, mode="replicate"):
        """
        åˆå§‹åŒ–DWTç±»ï¼Œå®šä¹‰å°æ³¢æ»¤æ³¢å™¨å’Œå…¶ä»–å‚æ•°ã€‚

        å‚æ•°:
        - dec_lo: ä½é¢‘æ»¤æ³¢å™¨
        - dec_hi: é«˜é¢‘æ»¤æ³¢å™¨
        - wavelet: å°æ³¢ç±»å‹ï¼ˆé»˜è®¤æ˜¯Haarå°æ³¢ï¼‰
        - level: å°æ³¢åˆ†è§£çš„å±‚æ•°
        - mode: å¡«å……æ¨¡å¼ï¼ˆé»˜è®¤ä¸º"replicate"ï¼‰
        """
        super(DWT, self).__init__()
        self.wavelet = _as_wavelet(wavelet)  # å°†waveletè½¬æ¢ä¸ºå°æ³¢å¯¹è±¡
        self.dec_lo = dec_lo  # ä½é¢‘æ»¤æ³¢å™¨
        self.dec_hi = dec_hi  # é«˜é¢‘æ»¤æ³¢å™¨
        self.level = level  # å°æ³¢åˆ†è§£çš„å±‚æ•°
        self.mode = mode  # å¡«å……æ¨¡å¼

    def forward(self, x):
        """
        æ‰§è¡Œå°æ³¢å˜æ¢ã€‚å°†è¾“å…¥å›¾åƒè¿›è¡Œå¤šå±‚å°æ³¢åˆ†è§£ã€‚

        å‚æ•°:
        - x: è¾“å…¥çš„å›¾åƒï¼ˆå½¢çŠ¶ä¸º [batch_size, channels, height, width]ï¼‰

        è¿”å›:
        - wavelet_component: å°æ³¢å˜æ¢åçš„ç»“æœï¼ŒåŒ…å«ä½é¢‘å’Œé«˜é¢‘åˆ†é‡
        """
        b, c, h, w = x.shape  # è·å–è¾“å…¥å›¾åƒçš„å°ºå¯¸
        if self.level is None:
            self.level = pywt.dwtn_max_level([h, w], self.wavelet)  # è‡ªåŠ¨è®¡ç®—æœ€å¤§åˆ†è§£å±‚æ•°

        # å­˜å‚¨æ¯ä¸€å±‚çš„å°æ³¢åˆ†é‡
        wavelet_component: List[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]] = []

        # åˆå§‹ä½é¢‘åˆ†é‡ä¸ºè¾“å…¥å›¾åƒ
        l_component = x
        # æ„é€ å°æ³¢å˜æ¢çš„å·ç§¯æ ¸
        dwt_kernel = construct_2d_filt(lo=self.dec_lo, hi=self.dec_hi)
        dwt_kernel = dwt_kernel.repeat(c, 1, 1)  # å¯¹æ¯ä¸ªé€šé“é‡å¤æ»¤æ³¢å™¨
        dwt_kernel = dwt_kernel.unsqueeze(dim=1)  # æ·»åŠ é¢å¤–çš„ç»´åº¦ä»¥ä¾¿è¿›è¡Œå·ç§¯æ“ä½œ

        # è¿›è¡Œå¤šå±‚å°æ³¢åˆ†è§£
        for _ in range(self.level):
            l_component = fwt_pad2(l_component, self.wavelet, mode=self.mode)  # å¡«å……å¹¶æ‰§è¡Œå‰å‘å°æ³¢å˜æ¢
            h_component = F.conv2d(l_component, dwt_kernel, stride=2, groups=c)  # æ‰§è¡Œå·ç§¯ä»¥è·å–é«˜é¢‘åˆ†é‡
            res = rearrange(h_component, 'b (c f) h w -> b c f h w', f=4)  # é‡æ’åˆ—ä»¥åˆ†ç¦»ä½é¢‘å’Œé«˜é¢‘åˆ†é‡
            l_component, lh_component, hl_component, hh_component = res.split(1, 2)  # åˆ†ç¦»ä½é¢‘å’Œä¸‰ä¸ªé«˜é¢‘åˆ†é‡
            # å°†é«˜é¢‘åˆ†é‡å­˜å‚¨åˆ°åˆ—è¡¨ä¸­
            wavelet_component.append((lh_component.squeeze(2), hl_component.squeeze(2), hh_component.squeeze(2)))

        # æ·»åŠ æœ€åçš„ä½é¢‘åˆ†é‡
        wavelet_component.append(l_component.squeeze(2))
        return wavelet_component[::-1]  # åè½¬é¡ºåºä»¥ä»é«˜é¢‘åˆ°ä½é¢‘è¿”å›ç»“æœ


# IDWTï¼ˆç¦»æ•£å°æ³¢é€†å˜æ¢ï¼‰æ¨¡å—
class IDWT(nn.Module):
    def __init__(self, rec_lo, rec_hi, wavelet='haar', level=1, mode="constant"):
        """
        åˆå§‹åŒ–IDWTç±»ï¼Œå®šä¹‰é€†å°æ³¢æ»¤æ³¢å™¨å’Œå…¶ä»–å‚æ•°ã€‚

        å‚æ•°:
        - rec_lo: ä½é¢‘é‡å»ºæ»¤æ³¢å™¨
        - rec_hi: é«˜é¢‘é‡å»ºæ»¤æ³¢å™¨
        - wavelet: å°æ³¢ç±»å‹ï¼ˆé»˜è®¤æ˜¯Haarå°æ³¢ï¼‰
        - level: å°æ³¢é€†å˜æ¢çš„å±‚æ•°
        - mode: å¡«å……æ¨¡å¼ï¼ˆé»˜è®¤ä¸º"constant"ï¼‰
        """
        super(IDWT, self).__init__()
        self.rec_lo = rec_lo  # ä½é¢‘é‡å»ºæ»¤æ³¢å™¨
        self.rec_hi = rec_hi  # é«˜é¢‘é‡å»ºæ»¤æ³¢å™¨
        self.wavelet = wavelet  # å°æ³¢ç±»å‹
        self.level = level  # å°æ³¢é€†å˜æ¢çš„å±‚æ•°
        self.mode = mode  # å¡«å……æ¨¡å¼

    def forward(self, x, weight=None):
        """
        æ‰§è¡Œå°æ³¢é€†å˜æ¢ã€‚æ ¹æ®è¾“å…¥çš„å°æ³¢åˆ†é‡é‡å»ºå›¾åƒã€‚

        å‚æ•°:
        - x: å°æ³¢åˆ†é‡åˆ—è¡¨ï¼ŒåŒ…æ‹¬ä½é¢‘å’Œé«˜é¢‘åˆ†é‡
        - weight: å¯é€‰çš„åŠ æƒå‚æ•°ï¼ˆé»˜è®¤ä¸ºNoneï¼Œè¡¨ç¤ºä½¿ç”¨è½¯æ­£äº¤ï¼‰

        è¿”å›:
        - l_component: é‡å»ºåçš„å›¾åƒ
        """
        l_component = x[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºåˆå§‹ä½é¢‘åˆ†é‡
        _, c, _, _ = l_component.shape  # è·å–é€šé“æ•°
        if weight is None:  # å¦‚æœæ²¡æœ‰æŒ‡å®šæƒé‡ï¼Œä½¿ç”¨è½¯æ­£äº¤
            idwt_kernel = construct_2d_filt(lo=self.rec_lo, hi=self.rec_hi)  # æ„é€ é€†å°æ³¢æ»¤æ³¢å™¨
            idwt_kernel = idwt_kernel.repeat(c, 1, 1)  # å¯¹æ¯ä¸ªé€šé“é‡å¤æ»¤æ³¢å™¨
            idwt_kernel = idwt_kernel.unsqueeze(dim=1)  # æ·»åŠ é¢å¤–çš„ç»´åº¦ä»¥ä¾¿è¿›è¡Œå·ç§¯æ“ä½œ
        else:  # å¦‚æœæŒ‡å®šäº†æƒé‡ï¼Œä½¿ç”¨ç¡¬æ­£äº¤
            idwt_kernel = torch.flip(weight, dims=[-1, -2])  # å¯¹æƒé‡è¿›è¡Œç¿»è½¬

        # è¿›è¡Œå°æ³¢é€†å˜æ¢
        self.filt_len = idwt_kernel.shape[-1]  # è·å–æ»¤æ³¢å™¨é•¿åº¦
        for c_pos, component_lh_hl_hh in enumerate(x[1:]):  # éå†æ¯ä¸€å±‚çš„å°æ³¢åˆ†é‡
            # å°†ä½é¢‘å’Œé«˜é¢‘åˆ†é‡æ‹¼æ¥æˆä¸€ä¸ªtensor
            l_component = torch.cat([l_component.unsqueeze(2), component_lh_hl_hh[0].unsqueeze(2),
                                     component_lh_hl_hh[1].unsqueeze(2), component_lh_hl_hh[2].unsqueeze(2)], 2)
            l_component = rearrange(l_component, 'b c f h w -> b (c f) h w')  # é‡æ’åˆ—

            # æ‰§è¡Œå·ç§¯è½¬ç½®ä»¥é‡å»ºå›¾åƒ
            l_component = F.conv_transpose2d(l_component, idwt_kernel, stride=2, groups=c)
            l_component = l_component.squeeze(1)  # å»é™¤å¤šä½™çš„ç»´åº¦

        return l_component  # è¿”å›é‡å»ºåçš„å›¾åƒ


class LWN(nn.Module):
    def __init__(self, dim, wavelet='haar', initialize=True, head=4, drop_rate=0., use_ca=False, use_sa=False):
        super(LWN, self).__init__()

        # åˆå§‹åŒ–å‚æ•°
        self.dim = dim  # è¾“å…¥ç‰¹å¾çš„é€šé“æ•°
        self.wavelet = _as_wavelet(wavelet)  # å°æ³¢å‡½æ•°ï¼Œè½¬æ¢ä¸ºå°æ³¢ç³»æ•°

        # è·å–å°æ³¢æ»¤æ³¢å™¨
        dec_lo, dec_hi, rec_lo, rec_hi = get_filter_tensors(wavelet, flip=True)

        # æ ¹æ®initializeé€‰æ‹©å¦‚ä½•åˆå§‹åŒ–å°æ³¢æ»¤æ³¢å™¨
        if initialize:
            self.dec_lo = nn.Parameter(dec_lo, requires_grad=True)  # åˆ†è§£ä½é¢‘æ»¤æ³¢å™¨
            self.dec_hi = nn.Parameter(dec_hi, requires_grad=True)  # åˆ†è§£é«˜é¢‘æ»¤æ³¢å™¨
            self.rec_lo = nn.Parameter(rec_lo.flip(-1), requires_grad=True)  # é‡å»ºä½é¢‘æ»¤æ³¢å™¨ï¼Œç¿»è½¬
            self.rec_hi = nn.Parameter(rec_hi.flip(-1), requires_grad=True)  # é‡å»ºé«˜é¢‘æ»¤æ³¢å™¨ï¼Œç¿»è½¬
        else:
            # éšæœºåˆå§‹åŒ–æ»¤æ³¢å™¨
            self.dec_lo = nn.Parameter(torch.rand_like(dec_lo) * 2 - 1, requires_grad=True)
            self.dec_hi = nn.Parameter(torch.rand_like(dec_hi) * 2 - 1, requires_grad=True)
            self.rec_lo = nn.Parameter(torch.rand_like(rec_lo) * 2 - 1, requires_grad=True)
            self.rec_hi = nn.Parameter(torch.rand_like(rec_hi) * 2 - 1, requires_grad=True)

        # å®šä¹‰DWTå’ŒIDWTæ¨¡å—
        self.wavedec = DWT(self.dec_lo, self.dec_hi, wavelet=wavelet, level=1)  # ç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰
        self.waverec = IDWT(self.rec_lo, self.rec_hi, wavelet=wavelet, level=1)  # é€†å°æ³¢å˜æ¢ï¼ˆIDWTï¼‰

        # å·ç§¯å±‚ï¼Œconv1å’Œconv2ç”¨æ¥æå–ç‰¹å¾
        self.conv1 = nn.Conv2d(dim * 4, dim * 6, 1)  # å·ç§¯1ï¼šä»å°æ³¢å˜æ¢ç»“æœçš„4ä¸ªåˆ†é‡æå–ç‰¹å¾
        self.conv2 = nn.Conv2d(dim * 6, dim * 6, 7, padding=3, groups=dim * 6)  # å·ç§¯2ï¼šæ·±åº¦å¯åˆ†ç¦»å·ç§¯
        self.act = nn.GELU()  # æ¿€æ´»å‡½æ•°
        self.conv3 = nn.Conv2d(dim * 6, dim * 4, 1)  # å·ç§¯3ï¼šå°†é€šé“æ•°å‹ç¼©å›åŸå§‹æ•°é‡

        # æ˜¯å¦ä½¿ç”¨ç©ºé—´æ³¨æ„åŠ›å’Œé€šé“æ³¨æ„åŠ›
        self.use_sa = use_sa
        self.use_ca = use_ca

        if self.use_sa:
            # å¦‚æœå¯ç”¨ç©ºé—´æ³¨æ„åŠ›ï¼Œå®šä¹‰æ°´å¹³å’Œå‚ç›´æ–¹å‘çš„ç©ºé—´æ³¨æ„åŠ›æ¨¡å—
            self.sa_h = nn.Sequential(
                nn.PixelShuffle(2),  # ä¸Šé‡‡æ ·
                nn.Conv2d(dim // 4, 1, kernel_size=1, padding=0, stride=1, bias=True)  # è¾“å‡ºé€šé“1
            )
            self.sa_v = nn.Sequential(
                nn.PixelShuffle(2),
                nn.Conv2d(dim // 4, 1, kernel_size=1, padding=0, stride=1, bias=True)
            )

        if self.use_ca:
            # å¦‚æœå¯ç”¨é€šé“æ³¨æ„åŠ›ï¼Œå®šä¹‰æ°´å¹³å’Œå‚ç›´æ–¹å‘çš„é€šé“æ³¨æ„åŠ›æ¨¡å—
            self.ca_h = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),  # å…¨å±€æ± åŒ–
                nn.Conv2d(dim, dim, 1, padding=0, stride=1, groups=1, bias=True),  # 1x1å·ç§¯
            )
            self.ca_v = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(dim, dim, 1, padding=0, stride=1, groups=1, bias=True)
            )
            self.shuffle = ShuffleBlock(2)  # ç”¨äºé€šé“é‡æ’çš„ShuffleBlock

    def forward(self, x):
        # è·å–è¾“å…¥å¼ é‡çš„å½¢çŠ¶
        _, _, H, W = x.shape

        # æ‰§è¡Œå°æ³¢åˆ†è§£ï¼Œå¾—åˆ°ä½é¢‘å’Œé«˜é¢‘åˆ†é‡
        ya, (yh, yv, yd) = self.wavedec(x)
        dec_x = torch.cat([ya, yh, yv, yd], dim=1)  # å°†å››ä¸ªåˆ†é‡æ‹¼æ¥èµ·æ¥

        # é€šè¿‡å·ç§¯ç½‘ç»œè¿›è¡Œç‰¹å¾æå–
        x = self.conv1(dec_x)
        x = self.conv2(x)
        x = self.act(x)
        x = self.conv3(x)

        # å°†è¾“å‡ºåˆ†æˆå››ä¸ªéƒ¨åˆ†
        ya, yh, yv, yd = torch.chunk(x, 4, dim=1)

        # æ‰§è¡Œå°æ³¢é‡å»º
        y = self.waverec([ya, (yh, yv, yd)], None)

        # å¦‚æœå¯ç”¨ç©ºé—´æ³¨æ„åŠ›ï¼Œè¿›è¡ŒåŠ æƒ
        if self.use_sa:
            sa_yh = self.sa_h(yh)
            sa_yv = self.sa_v(yv)
            y = y * (sa_yv + sa_yh)  # å°†åŠ æƒåçš„ç‰¹å¾ä¸è¾“å‡ºç›¸ä¹˜

        # å¦‚æœå¯ç”¨é€šé“æ³¨æ„åŠ›ï¼Œè¿›è¡ŒåŠ æƒ
        if self.use_ca:
            # é€šè¿‡ä¸Šé‡‡æ ·æ¢å¤è¾ƒå°çš„ç‰¹å¾å›¾
            yh = torch.nn.functional.interpolate(yh, scale_factor=2, mode='area')
            yv = torch.nn.functional.interpolate(yv, scale_factor=2, mode='area')

            # è®¡ç®—é€šé“æ³¨æ„åŠ›
            ca_yh = self.ca_h(yh)
            ca_yv = self.ca_v(yv)
            ca = self.shuffle(torch.cat([ca_yv, ca_yh], 1))  # é€šé“é‡æ’
            ca_1, ca_2 = ca.chunk(2, dim=1)  # åˆ†å‰²æˆä¸¤ä¸ªéƒ¨åˆ†
            ca = ca_1 * ca_2  # gated channel attention
            y = y * ca  # å°†åŠ æƒåçš„ç‰¹å¾ä¸è¾“å‡ºç›¸ä¹˜

        return y

    def get_wavelet_loss(self):
        # è¿”å›å°æ³¢é‡å»ºæŸå¤±
        return self.perfect_reconstruction_loss()[0] + self.alias_cancellation_loss()[0]

    def perfect_reconstruction_loss(self):
        """
        å®Œç¾é‡å»ºæŸå¤±ï¼šç¡®ä¿å°æ³¢åˆ†è§£å’Œé‡å»ºè¿‡ç¨‹èƒ½å¤Ÿå®Œç¾é‡å»ºåŸå§‹ä¿¡å·ã€‚
        ç†è®ºä¸Šï¼Œæ»¤æ³¢å™¨åº”è¯¥æ»¡è¶³P(z) + P(-z) = 2çš„æ¡ä»¶ã€‚è¿™é‡Œé‡‡ç”¨è½¯çº¦æŸã€‚
        """
        # è®¡ç®—P(z)çš„å¤šé¡¹å¼ä¹˜ç§¯
        pad = self.dec_lo.shape[-1] - 1
        p_lo = F.conv1d(
            self.dec_lo.flip(-1).unsqueeze(0),
            self.rec_lo.flip(-1).unsqueeze(0),
            padding=pad)
        pad = self.dec_hi.shape[-1] - 1
        p_hi = F.conv1d(
            self.dec_hi.flip(-1).unsqueeze(0),
            self.rec_hi.flip(-1).unsqueeze(0),
            padding=pad)

        p_test = p_lo + p_hi

        two_at_power_zero = torch.zeros(p_test.shape, device=p_test.device,
                                        dtype=p_test.dtype)
        two_at_power_zero[..., p_test.shape[-1] // 2] = 2
        # è®¡ç®—è¯¯å·®çš„å¹³æ–¹
        errs = (p_test - two_at_power_zero) * (p_test - two_at_power_zero)
        return torch.sum(errs), p_test, two_at_power_zero

    def alias_cancellation_loss(self):
        """
        alias cancellationæŸå¤±ï¼šç¡®ä¿å°æ³¢æ»¤æ³¢å™¨æ»¡è¶³F0(z)H0(-z) + F1(z)H1(-z) = 0çš„æ¡ä»¶
        """
        m1 = torch.tensor([-1], device=self.dec_lo.device, dtype=self.dec_lo.dtype)
        length = self.dec_lo.shape[-1]
        mask = torch.tensor([torch.pow(m1, n) for n in range(length)][::-1],
                            device=self.dec_lo.device, dtype=self.dec_lo.dtype)
        # è®¡ç®—å¤šé¡¹å¼ä¹˜ç§¯
        pad = self.dec_lo.shape[-1] - 1
        p_lo = torch.nn.functional.conv1d(
            self.dec_lo.flip(-1).unsqueeze(0) * mask,
            self.rec_lo.flip(-1).unsqueeze(0),
            padding=pad)

        pad = self.dec_hi.shape[-1] - 1
        p_hi = torch.nn.functional.conv1d(
            self.dec_hi.flip(-1).unsqueeze(0) * mask,
            self.rec_hi.flip(-1).unsqueeze(0),
            padding=pad)

        p_test = p_lo + p_hi
        zeros = torch.zeros(p_test.shape, device=p_test.device,
                            dtype=p_test.dtype)
        errs = (p_test - zeros) * (p_test - zeros)
        return torch.sum(errs), p_test, zeros


def autopad(k, p=None, d=1):  # kernel, padding, dilation
    """Pad to 'same' shape outputs."""
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p

class Conv(nn.Module):
    """Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)."""

    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        """Initialize Conv layer with given arguments including activation."""
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

    def forward(self, x):
        """Apply convolution, batch normalization and activation to input tensor."""
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        """Perform transposed convolution of 2D data."""
        return self.act(self.conv(x))

class Bottleneck_LWN(nn.Module):
    """Standard bottleneck."""

    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        """Initializes a standard bottleneck module with optional shortcut connection and configurable parameters."""
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, k[0], 1)
        self.cv2 = Conv(c_, c2, k[1], 1, g=g)
        self.cv3 = LWN(c2)
        self.add = shortcut and c1 == c2


    def forward(self, x):
        """Applies the YOLO FPN to input data."""
        return x + self.cv3(self.cv2(self.cv1(x))) if self.add else self.cv3(self.cv2(self.cv1(x)))

class C2f_LWN(nn.Module):
    """Faster Implementation of CSP Bottleneck with 2 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        """Initializes a CSP bottleneck with 2 convolutions and n Bottleneck blocks for faster processing."""
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.ModuleList(
            Bottleneck_LWN(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))

    def forward(self, x):
        """Forward pass through C2f layer."""
        y = list(self.cv1(x).chunk(2, 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))

    def forward_split(self, x):
        """Forward pass using split() instead of chunk()."""
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))




def main():
    # è®¾ç½®è¾“å…¥å‚æ•°
    dim = 64  # è¾“å…¥å›¾åƒé€šé“æ•°
    batch_size = 4  # æ‰¹å¤§å°
    height = 32  # è¾“å…¥å›¾åƒçš„é«˜åº¦
    width = 25  # è¾“å…¥å›¾åƒçš„å®½åº¦

    # åˆ›å»ºä¸€ä¸ªéšæœºè¾“å…¥å›¾åƒå¼ é‡
    x = torch.randn(batch_size, dim, height, width)

    # åˆå§‹åŒ–æ¨¡å‹
    fft2_model = LWN(dim=dim)
    # fft3_model = FFT3(dim=dim)

    # æ‰§è¡Œå‰å‘ä¼ æ’­
    output_fft2 = fft2_model(x)
    # output_fft3 = fft3_model(x)

    # æ‰“å°è¾“å‡ºçš„å½¢çŠ¶
    print("Input shape:", x.shape)
    print("Output shape (FFT2):", output_fft2.shape)
    # print("Output shape (FFT3):", output_fft3.shape)


if __name__ == "__main__":
    main()




# # Ultralytics YOLO ğŸš€, AGPL-3.0 license
# # YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
#
# # Parameters
# nc: 80 # number of classes
# scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
#   # [depth, width, max_channels]
#   n: [0.33, 0.25, 1024] # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
#   s: [0.33, 0.50, 1024] # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
#   m: [0.67, 0.75, 768] # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
#   l: [1.00, 1.00, 512] # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
#   x: [1.00, 1.25, 512] # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
#
# # YOLOv8.0n backbone
# backbone:
#   # [from, repeats, module, args]
#   - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2
#   - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4
#   - [-1, 3, C2f_WaveletConv, [128, True]]
#   - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8
#   - [-1, 6, C2f_WaveletConv, [256, True]]
#   - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16
#   - [-1, 6, C2f_WaveletConv, [512, True]]
#   - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32
#   - [-1, 3, C2f_WaveletConv, [1024, True]]
#   - [-1, 1, SPPF, [1024, 5]] # 9
#
# # YOLOv8.0n head
# head:
#   - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
#   - [[-1, 6], 1, Concat, [1]] # cat backbone P4
#   - [-1, 3, C2f, [512]] # 12
#
#   - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
#   - [[-1, 4], 1, Concat, [1]] # cat backbone P3
#   - [-1, 3, C2f, [256]] # 15 (P3/8-small)
#
#   - [-1, 1, Conv, [256, 3, 2]]
#   - [[-1, 12], 1, Concat, [1]] # cat head P4
#   - [-1, 3, C2f, [512]] # 18 (P4/16-medium)
#
#   - [-1, 1, Conv, [512, 3, 2]]
#   - [[-1, 9], 1, Concat, [1]] # cat head P5
#   - [-1, 3, C2f, [1024]] # 21 (P5/32-large)
#
#   - [[15, 18, 21], 1, Detect, [nc]] # Detect(P3, P4, P5)

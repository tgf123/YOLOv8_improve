import torch
import torch.nn as nn
import torch.nn.functional as F

# https://arxiv.org/pdf/2209.14145
# Multi-scal eAttentio nNetwork for Single Image Super-Resolution

class LayerNorm(nn.Module):
    r"""æ”¯æŒä¸¤ç§æ•°æ®æ ¼å¼çš„LayerNormï¼šchannels_lastï¼ˆé»˜è®¤ï¼‰æˆ–channels_firstã€‚
    è¾“å…¥çš„ç»´åº¦é¡ºåºã€‚channels_lastå¯¹åº”å½¢çŠ¶ä¸º(batch_size, height, width, channels)çš„è¾“å…¥ï¼Œ
    è€Œchannels_firstå¯¹åº”å½¢çŠ¶ä¸º(batch_size, channels, height, width)çš„è¾“å…¥ã€‚
    """

    def __init__(self, normalized_shape, eps=1e-6, data_format="channels_last"):
        super().__init__()
        # åˆå§‹åŒ–å‚æ•°ï¼ŒåŒ…æ‹¬æ ‡å‡†åŒ–å‚æ•°weightå’Œbias
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.data_format = data_format
        if self.data_format not in ["channels_last", "channels_first"]:
            raise NotImplementedError  # å¦‚æœæ ¼å¼æ— æ•ˆï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        self.normalized_shape = (normalized_shape,)

    def forward(self, x):
        # å¯¹äºchannels_lastæ ¼å¼ï¼Œä½¿ç”¨F.layer_normè¿›è¡Œæ ‡å‡†åŒ–
        if self.data_format == "channels_last":
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        # å¯¹äºchannels_firstæ ¼å¼ï¼Œæ‰‹åŠ¨è®¡ç®—å‡å€¼å’Œæ–¹å·®è¿›è¡Œæ ‡å‡†åŒ–
        elif self.data_format == "channels_first":
            u = x.mean(1, keepdim=True)  # è®¡ç®—å‡å€¼
            s = (x - u).pow(2).mean(1, keepdim=True)  # è®¡ç®—æ–¹å·®
            x = (x - u) / torch.sqrt(s + self.eps)  # æ ‡å‡†åŒ–
            x = self.weight[:, None, None] * x + self.bias[:, None, None]  # åŠ ä¸Šå¯å­¦ä¹ çš„æƒé‡å’Œåç½®
            return x


class MLKA_Ablation(nn.Module):
    def __init__(self, n_feats, k=2, squeeze_factor=15):
        super().__init__()
        i_feats = 2 * n_feats  # è¾“å…¥ç‰¹å¾ç»´åº¦æ˜¯n_featsçš„ä¸¤å€

        self.n_feats = n_feats
        self.i_feats = i_feats

        # åˆå§‹åŒ–å½’ä¸€åŒ–å±‚
        self.norm = LayerNorm(n_feats, data_format='channels_first')
        # ç”¨äºç¼©æ”¾çš„å¯å­¦ä¹ å‚æ•°
        self.scale = nn.Parameter(torch.zeros((1, n_feats, 1, 1)), requires_grad=True)

        # å®šä¹‰ä¸åŒå°ºåº¦çš„å¤§æ ¸æ³¨æ„åŠ›ï¼ˆLarge Kernel Attentionï¼‰æ¨¡å—
        # LKA7: ä½¿ç”¨7x7å·ç§¯ï¼Œ9x9å·ç§¯ï¼ˆè†¨èƒ€ä¸º4ï¼‰ï¼Œä»¥åŠ1x1å·ç§¯
        self.LKA7 = nn.Sequential(
            nn.Conv2d(n_feats // k, n_feats // k, 7, 1, 7 // 2, groups=n_feats // k),
            nn.Conv2d(n_feats // k, n_feats // k, 9, stride=1, padding=(9 // 2) * 4, groups=n_feats // k, dilation=4),
            nn.Conv2d(n_feats // k, n_feats // k, 1, 1, 0))

        # LKA5: ä½¿ç”¨5x5å·ç§¯ï¼Œ7x7å·ç§¯ï¼ˆè†¨èƒ€ä¸º3ï¼‰ï¼Œä»¥åŠ1x1å·ç§¯
        self.LKA5 = nn.Sequential(
            nn.Conv2d(n_feats // k, n_feats // k, 5, 1, 5 // 2, groups=n_feats // k),
            nn.Conv2d(n_feats // k, n_feats // k, 7, stride=1, padding=(7 // 2) * 3, groups=n_feats // k, dilation=3),
            nn.Conv2d(n_feats // k, n_feats // k, 1, 1, 0))

        # å…¶å®ƒæ¨¡å—ï¼Œæœªå¯ç”¨
        '''self.LKA3 = nn.Sequential(
            nn.Conv2d(n_feats//k, n_feats//k, 3, 1, 1, groups= n_feats//k),  
            nn.Conv2d(n_feats//k, n_feats//k, 5, stride=1, padding=(5//2)*2, groups=n_feats//k, dilation=2),
            nn.Conv2d(n_feats//k, n_feats//k, 1, 1, 0))'''

        # å®šä¹‰å…¶å®ƒå·ç§¯å±‚ï¼ˆä¸åŒå°ºå¯¸ï¼‰
        self.X5 = nn.Conv2d(n_feats // k, n_feats // k, 5, 1, 5 // 2, groups=n_feats // k)
        self.X7 = nn.Conv2d(n_feats // k, n_feats // k, 7, 1, 7 // 2, groups=n_feats // k)

        # ç”¨äºå°†è¾“å…¥ç‰¹å¾æ˜ å°„åˆ°æ›´é«˜ç»´åº¦çš„å·ç§¯å±‚
        self.proj_first = nn.Sequential(
            nn.Conv2d(n_feats, i_feats, 1, 1, 0))

        # ç”¨äºæœ€ç»ˆè¾“å‡ºæ˜ å°„å›n_featsç»´åº¦çš„å·ç§¯å±‚
        self.proj_last = nn.Sequential(
            nn.Conv2d(n_feats, n_feats, 1, 1, 0))

    def forward(self, x, pre_attn=None, RAA=None):
        # ä¿ç•™è¾“å…¥æ•°æ®ä½œä¸ºshortcutï¼Œè¿›è¡Œè·³è·ƒè¿æ¥
        shortcut = x.clone()

        # å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–
        x = self.norm(x)

        # æ˜ å°„åˆ°æ›´é«˜ç»´åº¦
        x = self.proj_first(x)

        # å°†ç‰¹å¾å›¾åˆ†æˆä¸¤éƒ¨åˆ†ï¼ˆaå’Œxï¼‰
        a, x = torch.chunk(x, 2, dim=1)

        # å°†aè¿›ä¸€æ­¥åˆ†æˆä¸¤éƒ¨åˆ†ï¼ˆa_1å’Œa_2ï¼‰
        a_1, a_2 = torch.chunk(a, 2, dim=1)

        # åˆ†åˆ«å¯¹a_1å’Œa_2è¿›è¡Œå¤šå°ºåº¦å¤§æ ¸æ³¨æ„åŠ›æ“ä½œï¼Œç»“åˆä¸åŒå·ç§¯ç»“æœ
        a = torch.cat([self.LKA7(a_1) * self.X7(a_1), self.LKA5(a_2) * self.X5(a_2)], dim=1)

        # æœ€åä¸€æ­¥å°†åŠ æƒåçš„xå’Œaåšå·ç§¯å˜æ¢ï¼Œå¹¶è¿›è¡Œè·³è·ƒè¿æ¥
        x = self.proj_last(x * a) * self.scale + shortcut

        return x


if __name__ == '__main__':
    #############Test Model Complexity #############
    # from fvcore.nn import flop_count_table, FlopCountAnalysis, ActivationCountAnalysis

    # åˆ›å»ºä¸€ä¸ªéšæœºè¾“å…¥å¼ é‡ï¼Œå¤§å°ä¸º (1, 256, 8, 8)
    x = torch.randn(1, 256, 8, 8)

    # åˆ›å»ºSAFMNPPæ¨¡å‹
    model = MLKA_Ablation(256)
    print(model)

    # æµ‹è¯•æ¨¡å‹çš„å‰å‘ä¼ æ’­
    output = model(x)
    print(output.shape)  # è¾“å‡ºçš„å½¢çŠ¶



# elif m is MLKA_Ablation:
#     args = [ch[f]]


# # Ultralytics YOLO ğŸš€, AGPL-3.0 license
# # YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
#
# # Parameters
# nc: 80 # number of classes
# scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
#   # [depth, width, max_channels]
#   n: [0.33, 0.25, 1024] # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
#   s: [0.33, 0.50, 1024] # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
#   m: [0.67, 0.75, 768] # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
#   l: [1.00, 1.00, 512] # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
#   x: [1.00, 1.25, 512] # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
#
# # YOLOv8.0n backbone
# backbone:
#   # [from, repeats, module, args]
#   - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2
#   - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4
#   - [-1, 3, C2f, [128, True]]
#   - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8
#   - [-1, 6, C2f, [256, True]]
#   - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16
#   - [-1, 6, C2f, [512, True]]
#   - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32
#   - [-1, 3, C2f, [1024, True]]
#   - [-1, 1, MLKA_Ablation, []] # 9
#
# # YOLOv8.0n head
# head:
#   - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
#   - [[-1, 6], 1, Concat, [1]] # cat backbone P4
#   - [-1, 3, C2f, [512]] # 12
#
#   - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
#   - [[-1, 4], 1, Concat, [1]] # cat backbone P3
#   - [-1, 3, C2f, [256]] # 15 (P3/8-small)
#
#   - [-1, 1, Conv, [256, 3, 2]]
#   - [[-1, 12], 1, Concat, [1]] # cat head P4
#   - [-1, 3, C2f, [512]] # 18 (P4/16-medium)
#
#   - [-1, 1, Conv, [512, 3, 2]]
#   - [[-1, 9], 1, Concat, [1]] # cat head P5
#   - [-1, 3, C2f, [1024]] # 21 (P5/32-large)
#
#   - [[15, 18, 21], 1, Detect, [nc]] # Detect(P3, P4, P5)

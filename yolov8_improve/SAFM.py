import torch
import torch.nn as nn
import torch.nn.functional as F


# è®ºæ–‡åœ°å€ï¼šhttps://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Spatially-Adaptive_Feature_Modulation_for_Efficient_Image_Super-Resolution_ICCV_2023_paper.pdf
# è®ºæ–‡é¢˜ç›®ï¼šSpatially-Adaptive Feature Modulation for Efficient Image Super-Resolution
# è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç©ºé—´è‡ªé€‚åº”ç‰¹å¾è°ƒåˆ¶ï¼ˆSAFMï¼‰æ–¹æ³•æ¥æœ‰æ•ˆåœ°è¿›è¡Œå›¾åƒè¶…åˆ†è¾¨ç‡

class SimpleSAFM(nn.Module):
    def __init__(self, dim, ratio=4):
        """
        åˆå§‹åŒ–SimpleSAFMæ¨¡å—ã€‚
        dim: è¾“å…¥çš„ç‰¹å¾å›¾é€šé“æ•°ã€‚
        ratio: å°†é€šé“æ•°åˆ†ä¸ºä¸¤éƒ¨åˆ†çš„æ¯”ä¾‹ã€‚
        """
        super().__init__()
        self.dim = dim
        self.chunk_dim = dim // ratio  # å°†è¾“å…¥é€šé“æ•°åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†

        # å·ç§¯å±‚ï¼Œç”¨äºç‰¹å¾æ˜ å°„çš„æŠ•å½±
        self.proj = nn.Conv2d(dim, dim, 3, 1, 1, bias=False)

        # æ·±åº¦å·ç§¯ï¼Œç”¨äºå¤„ç†åˆ†å‰²åçš„ç‰¹å¾
        self.dwconv = nn.Conv2d(self.chunk_dim, self.chunk_dim, 3, 1, 1, groups=self.chunk_dim, bias=False)

        # è¾“å‡ºå·ç§¯å±‚
        self.out = nn.Conv2d(dim, dim, 1, 1, 0, bias=False)

        # æ¿€æ´»å‡½æ•°
        self.act = nn.GELU()

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šè¿›è¡Œç‰¹å¾æå–ä¸è°ƒåˆ¶
        x: è¾“å…¥ç‰¹å¾å›¾ï¼Œå¤§å°ä¸º (batch_size, dim, H, W)
        """
        h, w = x.size()[-2:]  # è·å–è¾“å…¥çš„é«˜åº¦å’Œå®½åº¦

        # å°†è¾“å…¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œx0 å’Œ x1
        x0, x1 = self.proj(x).split([self.chunk_dim, self.dim - self.chunk_dim], dim=1)

        # å¯¹x0è¿›è¡Œæ± åŒ–ï¼Œå¹¶é€šè¿‡æ·±åº¦å·ç§¯è¿›è¡Œå¤„ç†
        x2 = F.adaptive_max_pool2d(x0, (h // 8, w // 8))  # è‡ªé€‚åº”æ± åŒ–ï¼Œç¼©å°ä¸ºåŸå›¾çš„1/8å¤§å°
        x2 = self.dwconv(x2)  # æ·±åº¦å·ç§¯
        x2 = F.interpolate(x2, size=(h, w), mode='bilinear')  # ä¸Šé‡‡æ ·æ¢å¤åˆ°åŸå°ºå¯¸
        x2 = self.act(x2) * x0  # æ¿€æ´»å¹¶ä¸x0ç›¸ä¹˜ï¼Œè¿›è¡Œç‰¹å¾è°ƒåˆ¶

        # å°†x1å’Œè°ƒåˆ¶åçš„x2åˆå¹¶ï¼Œè¿›è¡Œåç»­å¤„ç†
        x = torch.cat([x1, x2], dim=1)
        x = self.out(self.act(x))  # è¾“å‡º
        return x


# Convolutional Channel Mixer æ¨¡å—
class CCM(nn.Module):
    def __init__(self, dim, ffn_scale, use_se=False):
        """
        åˆå§‹åŒ–CCMæ¨¡å—ã€‚
        dim: è¾“å…¥çš„ç‰¹å¾å›¾é€šé“æ•°ã€‚
        ffn_scale: ç”¨äºè®¡ç®—éšè—å±‚ç»´åº¦çš„æ¯”ä¾‹ã€‚
        use_se: æ˜¯å¦ä½¿ç”¨Squeeze-and-Excitationï¼ˆSEï¼‰æ¨¡å—
        """
        super().__init__()
        self.use_se = use_se
        hidden_dim = int(dim * ffn_scale)  # éšè—å±‚é€šé“æ•°

        # å·ç§¯å±‚1ï¼šè¿›è¡Œé€šé“æ‰©å±•
        self.conv1 = nn.Conv2d(dim, hidden_dim, 3, 1, 1, bias=False)

        # å·ç§¯å±‚2ï¼šå°†é€šé“æ¢å¤åˆ°åŸå§‹ç»´åº¦
        self.conv2 = nn.Conv2d(hidden_dim, dim, 1, 1, 0, bias=False)

        self.act = nn.GELU()  # æ¿€æ´»å‡½æ•°

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šè¿›è¡Œç‰¹å¾æ··åˆ
        x: è¾“å…¥ç‰¹å¾å›¾ï¼Œå¤§å°ä¸º (batch_size, dim, H, W)
        """
        x = self.act(self.conv1(x))  # ç»è¿‡å·ç§¯å’Œæ¿€æ´»
        x = self.conv2(x)  # ç»è¿‡ç¬¬äºŒå±‚å·ç§¯
        return x


# Attention Block æ¨¡å—
class AttBlock(nn.Module):
    def __init__(self, dim, ffn_scale, use_se=False):
        """
        åˆå§‹åŒ–Attention Blockæ¨¡å—ï¼Œè¯¥æ¨¡å—åŒ…å«SimpleSAFMå’ŒCCM
        """
        super().__init__()

        # SimpleSAFMï¼šç©ºé—´è‡ªé€‚åº”ç‰¹å¾è°ƒåˆ¶
        self.conv1 = SimpleSAFM(dim, ratio=3)

        # CCMï¼šå·ç§¯é€šé“æ··åˆ
        self.conv2 = CCM(dim, ffn_scale, use_se)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šä¾æ¬¡ç»è¿‡SimpleSAFMå’ŒCCMæ¨¡å—
        x: è¾“å…¥ç‰¹å¾å›¾ï¼Œå¤§å°ä¸º (batch_size, dim, H, W)
        """
        out = self.conv1(x)  # ç»è¿‡SimpleSAFMæ¨¡å—
        out = self.conv2(out)  # ç»è¿‡CCMæ¨¡å—
        return out + x  # æ®‹å·®è¿æ¥


# SAFMä¸NPPï¼ˆNon-Local Prior Poolingï¼‰ç»“åˆçš„è¶…åˆ†è¾¨ç‡ç½‘ç»œ
class SAFMNPP(nn.Module):
    def __init__(self, input_dim, dim, n_blocks=3, ffn_scale=1.5, use_se=False, upscaling_factor=2):
        """
        åˆå§‹åŒ–SAFMNPPè¶…åˆ†è¾¨ç‡æ¨¡å‹ã€‚
        input_dim: è¾“å…¥å›¾åƒçš„é€šé“æ•°ã€‚
        dim: ç½‘ç»œä¸­é—´å±‚çš„é€šé“æ•°ã€‚
        n_blocks: æ³¨æ„åŠ›å—çš„æ•°é‡ã€‚
        ffn_scale: ç”¨äºè®¡ç®—é€šé“æ•°çš„æ¯”ä¾‹ã€‚
        use_se: æ˜¯å¦ä½¿ç”¨Squeeze-and-Excitationï¼ˆSEï¼‰æ¨¡å—ã€‚
        upscaling_factor: è¶…åˆ†è¾¨ç‡æ”¾å¤§å› å­ã€‚
        """
        super().__init__()
        self.scale = upscaling_factor  # è®¾ç½®ä¸Šé‡‡æ ·å› å­

        # è¾“å…¥ç‰¹å¾å›¾è½¬æ¢
        self.to_feat = nn.Conv2d(input_dim, dim, 3, 1, 1, bias=False)

        # å †å å¤šä¸ªAttentionå—
        self.feats = nn.Sequential(*[AttBlock(dim, ffn_scale, use_se) for _ in range(n_blocks)])

        # è¾“å‡ºæ¢å¤å›¾åƒçš„å·ç§¯å±‚
        self.to_img = nn.Sequential(
            nn.Conv2d(dim, input_dim * upscaling_factor ** 2, 3, 1, 1, bias=False),
            nn.PixelShuffle(upscaling_factor)  # ç”¨PixelShuffleè¿›è¡Œä¸Šé‡‡æ ·
        )

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šè¾“å…¥å›¾åƒç»è¿‡å¤šä¸ªæ¨¡å—è¿›è¡Œè¶…åˆ†è¾¨ç‡æ¢å¤
        x: è¾“å…¥å›¾åƒï¼Œå¤§å°ä¸º (batch_size, input_dim, H, W)
        """
        # é¦–å…ˆé€šè¿‡æ’å€¼è¿›è¡Œä¸Šé‡‡æ ·
        res = F.interpolate(x, scale_factor=self.scale, mode='bilinear', align_corners=False)

        # é€šè¿‡to_featæ¨¡å—å°†è¾“å…¥ç‰¹å¾è½¬æ¢åˆ°ä¸­é—´å±‚
        x = self.to_feat(x)

        # é€šè¿‡å¤šä¸ªAttentionå—è¿›è¡Œç‰¹å¾å¤„ç†
        x = self.feats(x)

        # é€šè¿‡to_imgæ¨¡å—è¿›è¡Œæ¢å¤å›¾åƒ
        return self.to_img(x) + res  # æœ€ç»ˆè¾“å‡ºå›¾åƒåŠ ä¸Šæ®‹å·®


# æµ‹è¯•æ¨¡å‹çš„å¤æ‚åº¦
if __name__ == '__main__':
    #############Test Model Complexity #############
    # from fvcore.nn import flop_count_table, FlopCountAnalysis, ActivationCountAnalysis

    # åˆ›å»ºä¸€ä¸ªéšæœºè¾“å…¥å¼ é‡ï¼Œå¤§å°ä¸º (1, 256, 8, 8)
    x = torch.randn(1, 256, 8, 8)

    # åˆ›å»ºSAFMNPPæ¨¡å‹
    model = SAFMNPP(256, dim=256, n_blocks=6, ffn_scale=1.5, upscaling_factor=2)
    print(model)

    # æµ‹è¯•æ¨¡å‹çš„å‰å‘ä¼ æ’­
    output = model(x)
    print(output.shape)  # è¾“å‡ºçš„å½¢çŠ¶

# # Ultralytics YOLO ğŸš€, AGPL-3.0 license
# # YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
#
# # Parameters
# nc: 80 # number of classes
# scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
#   # [depth, width, max_channels]
#   n: [0.33, 0.25, 1024] # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
#   s: [0.33, 0.50, 1024] # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
#   m: [0.67, 0.75, 768] # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
#   l: [1.00, 1.00, 512] # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
#   x: [1.00, 1.25, 512] # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
#
# # YOLOv8.0n backbone
# backbone:
#   # [from, repeats, module, args]
#   - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2
#   - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4
#   - [-1, 3, C2f, [128, True]]
#   - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8
#   - [-1, 6, C2f, [256, True]]
#   - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16
#   - [-1, 6, C2f, [512, True]]
#   - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32
#   - [-1, 3, C2f, [1024, True]]
#   - [-1, 1, SPPF, [1024, 5]] # 9
#
# # YOLOv8.0n head
# head:
#   - [-1, 1, nn.Upsample, [None, 2, "nearest"]]
#   - [[-1, 6], 1, Concat, [1]] # cat backbone P4
#   - [-1, 3, C2f, [512]] # 12
#
#   - [-1, 1, SAFMNPP, [512]]
#   - [[-1, 4], 1, Concat, [1]] # cat backbone P3
#   - [-1, 3, C2f, [256]] # 15 (P3/8-small)
#
#   - [-1, 1, Conv, [256, 3, 2]]
#   - [[-1, 12], 1, Concat, [1]] # cat head P4
#   - [-1, 3, C2f, [512]] # 18 (P4/16-medium)
#
#   - [-1, 1, Conv, [512, 3, 2]]
#   - [[-1, 9], 1, Concat, [1]] # cat head P5
#   - [-1, 3, C2f, [1024]] # 21 (P5/32-large)
#
#   - [[15, 18, 21], 1, Detect, [nc]] # Detect(P3, P4, P5)
